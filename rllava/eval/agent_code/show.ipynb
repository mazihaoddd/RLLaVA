{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "system_prompt = \"\"\"# Role\n",
    "You are a step-by-step image processing assistant.\n",
    "Your task is to solve an image-based task by applying OpenCV operations one step at a time, optionally using a reasoning chain.\n",
    "\n",
    "# Output Format\n",
    "At each step, output **only one** of the following, preceded by a <think> tag:\n",
    "1. <problem> Describe the image issue from {'rotation90', 'rotation180', 'dark', 'overexposure', 'blur', 'noise', 'crop', 'none'} </problem>\n",
    "2. <code> OpenCV code to process and save the image </code>\n",
    "3. <answer> Final answer based on the processed image </answer>\n",
    "\n",
    "# Image Processing Rules\n",
    "- Always read from `'path_to_input_image.jpg'` and write to `'path_to_output_image.jpg'`.\n",
    "\n",
    "# Output Format (strict):\n",
    "Always begin with <think>. Then, depending on current reasoning chain, output one of the following:\n",
    "\n",
    "## 1. If this is the first step and only the query is given, output in the following format:\n",
    "<think> Initial analysis of the image issue. </think>\n",
    "<problem> {'problem1', ...} </problem>\n",
    "\n",
    "## 2. If <problem> is given, continue with image operations:\n",
    "<think> Explain what to fix next. </think>\n",
    "<code>\n",
    "```python\n",
    "One Python code block using OpenCV to perform the operation, and save the processed images.\n",
    "```\n",
    "</code>\n",
    "\n",
    "## 3. If ready to conclude:\n",
    "<think> Summarize the processing steps and provide the result or outcome </think> \n",
    "<answer> Final answer, as briefly as possible</answer>\n",
    "\n",
    "# Current reasoning chain:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = '/mnt/ve_share/zhaolei/outputs/code/tinyllava-qwen2.5-vl-3b-agent-code/checkpoint-600'\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    device_map='cuda:0',\n",
    ")\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "json_path = \"/root/code/obs_mining/tmp/MAT-Coding.json\"\n",
    "\n",
    "with open(json_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in tqdm(data):\n",
    "    print(\"########################################\")\n",
    "    if item['type'][0] == 'crop':\n",
    "        input_image_path = item['ori_image_path']\n",
    "    else:\n",
    "        input_image_path = item['processed_image_path']\n",
    "\n",
    "    input_image_path = '/mnt/ve_share/zhaolei/.cache/huggingface/datasets/laolao77___MAT/MAT-Benchmark/MAT-Coding-image/' + input_image_path\n",
    "    query = item['question']\n",
    "    data_type = item['type']\n",
    "    item_id = item['id']\n",
    "\n",
    "    output_image_path = 'cache.jpg'\n",
    "    input_text = system_prompt + f'<query> {query} <query>'\n",
    "    print(f'<query> {query} </query>')\n",
    "\n",
    "    messages = [\n",
    "        { \"role\": \"user\", \n",
    "            \"content\": [{\"type\": \"image\",\"image\": input_image_path}, {\"type\": \"text\", \"text\": input_text}]}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "r1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
